# this must be provided and if not provided, it is experiment-01
run_name: experiment-01
##
run_description: "this is a demo run"
# here is my model and data
# it depends on the project
data:
  max_len: 50
  vocab_n: 33
  data_n: 320
  batch_size: 16
model:
  type: "TransformerForConditionalLLM"
  vocab_n: 33
  num_layers: 2
  d: 512
  n: 8
  max_len: 50
  d_ff: 2048
  dropout: 0.1
  use_rope: true
# below list all the config of runner with the default value
# you can change it if you want
optimizer:
  lr: 3e-4
log:
  log_level: "INFO"
  to_file: true
  folder: "./logs"
scheduler:
  type: "LinearWarmupCosineLRScheduler"
  min_lr: 2e-5
  max_lr: 3e-4
  warmup_rate: 0.05
  warmup_start_lr: 1e-5
pt:
  pt_save: true
  pt_save_dir: "./checkpoints"
  pt_save_n_epochs: 1
  pt_best_monitor:
    loss: false
  pt_topk: 3
wandb:
  wandb_enable: true
  wandb_project_name: "default"
  wandb_offline: false
  wandb_dir: "./"
  wandb_tags: []
training:
  epochs: 10
  print_model: true
  fp16: false
  progress_show:
    loss: false
    valid_acc: true
    test_acc: true
  valid_every_n_epochs: 1
  test_every_n_epochs: 1
  progress_every_n_epochs: 1
  progress_every_n_batches: 1
  ds_config: null